"""
Linear Regression Model with Gradient Descent
==============================================
Course: Supervised Learning - Linear Regression

Author: Yeabsira Samuel
Currency: Ethiopian Birr (ETB)

THEORY:
-------
Linear Regression finds the best-fit line through data points.

Model Equation:
    ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô
    
    Where:
    ≈∑ = predicted bill (ETB)
    Œ≤‚ÇÄ = intercept (base cost)
    Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô = coefficients (weights)
    x‚ÇÅ, x‚ÇÇ, ..., x‚Çô = features (hours of usage)

Gradient Descent Algorithm:
    1. Start with random weights (Œ≤)
    2. Calculate predictions: ≈∑ = XŒ≤
    3. Calculate error (cost): J = (1/2m) Œ£(≈∑ - y)¬≤
    4. Update weights: Œ≤ = Œ≤ - Œ±(‚àÇJ/‚àÇŒ≤)
    5. Repeat until convergence

Cost Function (Mean Squared Error):
    J(Œ≤) = (1/2m) Œ£(≈∑·µ¢ - y·µ¢)¬≤
    
    Where:
    m = number of samples
    ≈∑·µ¢ = predicted value
    y·µ¢ = actual value
"""

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pickle


class LinearRegressionModel:
    """
    Linear Regression Model with Gradient Descent
    
    This class implements Linear Regression for predicting electricity bills
    from appliance usage patterns.
    """
    
    def __init__(self):
        """Initialize the Linear Regression model"""
        self.model = LinearRegression()
        self.feature_names = None
        self.is_trained = False
        
    def train(self, X_train, y_train, feature_names=None):
        """
        Train the Linear Regression model
        
        Parameters:
        -----------
        X_train : numpy array
            Training features (scaled)
        y_train : numpy array
            Training targets (bills in ETB)
        feature_names : list
            Names of features
        """
        
        print("\n" + "="*70)
        print("üéì LINEAR REGRESSION THEORY")
        print("="*70)
        
        print("\nüìê MODEL EQUATION:")
        print("   ≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô")
        print()
        print("   Where:")
        print("   ‚Ä¢ ≈∑ = predicted bill (ETB)")
        print("   ‚Ä¢ Œ≤‚ÇÄ = intercept (base cost)")
        print("   ‚Ä¢ Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô = coefficients (impact of each feature)")
        print("   ‚Ä¢ x‚ÇÅ, x‚ÇÇ, ..., x‚Çô = features (appliance usage hours)")
        
        print("\nüéØ GRADIENT DESCENT ALGORITHM:")
        print("="*70)
        print("Goal: Find Œ≤ that minimizes prediction error")
        print()
        print("Steps:")
        print("   1Ô∏è‚É£  Initialize: Start with random weights Œ≤")
        print("   2Ô∏è‚É£  Predict: Calculate ≈∑ = XŒ≤")
        print("   3Ô∏è‚É£  Error: Calculate cost J = (1/2m)Œ£(≈∑ - y)¬≤")
        print("   4Ô∏è‚É£  Update: Œ≤ = Œ≤ - Œ±(‚àÇJ/‚àÇŒ≤)")
        print("   5Ô∏è‚É£  Repeat: Until cost stops decreasing")
        print()
        print("Where:")
        print("   ‚Ä¢ Œ± (alpha) = learning rate (step size)")
        print("   ‚Ä¢ ‚àÇJ/‚àÇŒ≤ = gradient (direction of steepest descent)")
        print("   ‚Ä¢ m = number of training samples")
        
        print("\nüìä COST FUNCTION (Mean Squared Error):")
        print("="*70)
        print("   J(Œ≤) = (1/2m) Œ£(≈∑·µ¢ - y·µ¢)¬≤")
        print()
        print("   This measures how far predictions are from actual values.")
        print("   Gradient Descent minimizes this cost function.")
        
        print("\nüîÑ TRAINING IN PROGRESS...")
        print("="*70)
        
        # Train the model
        self.model.fit(X_train, y_train)
        self.feature_names = feature_names
        self.is_trained = True
        
        # Get learned parameters
        intercept = self.model.intercept_
        coefficients = self.model.coef_
        
        print("‚úÖ Training completed!")
        
        print("\nüìä LEARNED PARAMETERS:")
        print("="*70)
        print(f"   Œ≤‚ÇÄ (Intercept): {intercept:.2f} ETB")
        print()
        print("   Coefficients (Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çô):")
        if feature_names:
            for i, (name, coef) in enumerate(zip(feature_names, coefficients), 1):
                emoji = "üî¥" if abs(coef) > 100 else "üü°" if abs(coef) > 50 else "üü¢"
                print(f"      {emoji} Œ≤{i} ({name:<20}): {coef:>8.2f}")
        else:
            for i, coef in enumerate(coefficients, 1):
                print(f"      Œ≤{i}: {coef:.2f}")
        
        print("\nüí° Interpretation:")
        print("   ‚Ä¢ Positive coefficient = feature increases bill")
        print("   ‚Ä¢ Negative coefficient = feature decreases bill")
        print("   ‚Ä¢ Larger |coefficient| = stronger impact on bill")
        
    def predict(self, X):
        """
        Make predictions
        
        Parameters:
        -----------
        X : numpy array
            Features to predict on
            
        Returns:
        --------
        predictions : numpy array
            Predicted bills (ETB)
        """
        if not self.is_trained:
            raise ValueError("Model not trained! Call train() first.")
        
        return self.model.predict(X)
    
    def evaluate(self, X_test, y_test):
        """
        Evaluate model performance
        
        Parameters:
        -----------
        X_test : numpy array
            Testing features
        y_test : numpy array
            Actual bills (ETB)
            
        Returns:
        --------
        metrics : dict
            R¬≤, RMSE, MAE
        """
        
        print("\n" + "="*70)
        print("üìä MODEL EVALUATION")
        print("="*70)
        
        # Make predictions
        y_pred = self.predict(X_test)
        
        # Calculate metrics
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        
        print("\nüìà Performance Metrics:")
        print("-"*70)
        
        # R¬≤ Score
        print(f"\n   R¬≤ Score: {r2:.4f}")
        print("   ‚îú‚îÄ Interpretation: Percentage of variance explained")
        if r2 > 0.9:
            print("   ‚îî‚îÄ ‚≠ê‚≠ê‚≠ê Excellent fit!")
        elif r2 > 0.7:
            print("   ‚îî‚îÄ ‚≠ê‚≠ê Good fit!")
        elif r2 > 0.5:
            print("   ‚îî‚îÄ ‚≠ê Acceptable fit")
        else:
            print("   ‚îî‚îÄ ‚ö†Ô∏è  Poor fit - consider more features")
        
        # RMSE
        print(f"\n   RMSE: {rmse:.2f} ETB")
        print("   ‚îú‚îÄ Interpretation: Average prediction error (penalizes large errors)")
        print(f"   ‚îî‚îÄ On average, predictions are off by ¬±{rmse:.2f} ETB")
        
        # MAE
        print(f"\n   MAE: {mae:.2f} ETB")
        print("   ‚îú‚îÄ Interpretation: Average absolute error")
        print(f"   ‚îî‚îÄ Typical error: {mae:.2f} ETB")
        
        # Compare with baseline
        baseline_error = np.mean(np.abs(y_test - np.mean(y_test)))
        improvement = ((baseline_error - mae) / baseline_error) * 100
        
        print(f"\n   Baseline (predicting mean): {baseline_error:.2f} ETB")
        print(f"   Improvement: {improvement:.1f}%")
        
        print("\n" + "="*70)
        
        return {
            'r2': r2,
            'rmse': rmse,
            'mae': mae
        }
    
    def save(self, filepath='models/linear_regression_model.pkl'):
        """Save trained model to file"""
        if not self.is_trained:
            raise ValueError("Cannot save untrained model!")
        
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)
        
        print(f"\n‚úÖ Model saved: {filepath}")
    
    @staticmethod
    def load(filepath='models/linear_regression_model.pkl'):
        """Load trained model from file"""
        with open(filepath, 'rb') as f:
            model = pickle.load(f)
        
        print(f"‚úÖ Model loaded: {filepath}")
        return model
    
    def get_feature_importance(self):
        """
        Get feature importance based on absolute coefficient values
        
        Returns:
        --------
        importance : dict
            Feature names and their importance scores
        """
        if not self.is_trained:
            raise ValueError("Model not trained!")
        
        if self.feature_names is None:
            return None
        
        # Absolute value of coefficients
        importance = {
            name: abs(coef) 
            for name, coef in zip(self.feature_names, self.model.coef_)
        }
        
        # Sort by importance
        importance = dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))
        
        return importance


if __name__ == "__main__":
    # Test the model
    print("Linear Regression Model Module")
    print("Run train.py to train the model")